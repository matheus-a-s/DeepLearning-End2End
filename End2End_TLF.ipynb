{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Consult GPU, or CPU, disponibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU == True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.is_gpu_available() == True:\n",
    "    config = tf.compat.v1.ConfigProto(\n",
    "        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "        # device_count = {'GPU': 1}\n",
    "    )\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=config)\n",
    "    tf.keras.backend.set_session(session)\n",
    "\n",
    "print(\"Using GPU ==\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Configure Deep Transfer Learning Framework - USER CONFIGURATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform path of dataset (separate folders per numeric class)\n",
    "srcPath = './dataset/DDSM_ROI/mass&B2&B5_augmentation'          \n",
    "\n",
    "# Use Images-ID? (Images can have an ID to verify, at end of classification what images are contributing to error)\n",
    "ImagesID = False # NOTE: ID of images may be integer and the first argument at file name (sep='_'). Ex.: \"123_mirrored.png\"\n",
    "# NOTE 2: If save features as CSV file, image ID can be a string. If images IDs are string type, set ImagesID to False.\n",
    "\n",
    "\n",
    "# CONFIGURE CLASSES\n",
    "classes = [2,5] # Rename your folders classes like this\n",
    "class_weight = {2:1,5:1} # Classes weights\n",
    "set_split = {'total_per_class': [680,680], # Total of images per class\n",
    "             'test_set': [130,130], # Images for test\n",
    "             'train_set': [680-130,680-130]}\n",
    "\n",
    "\n",
    "# CONFIGURE EXTRACTION\n",
    "# All CNN's architetures to use. (NOTE: NASNetMobile and NASNetLarge are broken)\n",
    "model_name_list = ['MobileNet','ResNet50','Xception','VGG16','VGG19','InceptionV3','InceptionResNetV2', \\\n",
    "                   'DenseNet121','DenseNet169','DenseNet201','NASNetMobile','NASNetLarge'] \n",
    "\n",
    "# Architetures chosen\n",
    "model_name_list = ['VGG16'] # Choose CNN's of your preference\n",
    "\n",
    "# Saving features format.\n",
    "output_fmt = ['txt', 'npy', 'csv']\n",
    "\n",
    "\n",
    "# CONFIGURE CLASSIFICATION\n",
    "# All classifiers to use.\n",
    "classifiers_name_list = ['Bayes','MLP','Nearest_Neighbors','Random_Forest','SVM_Linear','SVM_Polynomial','SVM_RBF']\n",
    "\n",
    "# Classifiers chosen\n",
    "classifiers_name_list = ['Nearest_Neighbors','SVM_Linear','SVM_RBF'] # Choose classifiers of your preference\n",
    "\n",
    "# All metrics to use.\n",
    "metrics_name_list = ['accuracy', 'balanced_accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score']\n",
    "\n",
    "# Metrics chosen\n",
    "metrics_name_list = ['accuracy', 'balanced_accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score'] # Choose metrics of your preference\n",
    "\n",
    "# Define number of rounds of classification (to generate mean and std of metrics)\n",
    "classification_rounds = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import keras\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from scipy import misc\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import DenseNet169\n",
    "from keras.applications.densenet import DenseNet201\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input as ppi_inceptionresnet_v2\n",
    "from keras.applications.vgg16 import preprocess_input as ppi_vgg16\n",
    "from keras.applications.vgg19 import preprocess_input as ppi_vgg19\n",
    "from keras.applications.xception import preprocess_input as ppi_xception\n",
    "from keras.applications.resnet50 import preprocess_input as ppi_resnet50\n",
    "from keras.applications.inception_v3 import preprocess_input as ppi_inception_v3\n",
    "from keras.applications.mobilenet import preprocess_input as ppi_mobilenet\n",
    "from keras.applications.nasnet import preprocess_input as ppi_nasnet\n",
    "from keras.applications.densenet import preprocess_input as ppi_densenet\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from imblearn.metrics import sensitivity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Extraction functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model, without the last fully-connected layers\n",
    "def build_model(model_name, pooltype='max'):\n",
    "    if ('VGG16' is model_name):\n",
    "        model = VGG16(weights='imagenet', pooling=pooltype, include_top=False)    \n",
    "    if ('VGG19' is model_name):\n",
    "        model = VGG19(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('MobileNet' is model_name):\n",
    "        model = MobileNet(weights='imagenet', pooling=pooltype, input_shape=(224,224,3), include_top=False)  \n",
    "    if ('ResNet50' is model_name):\n",
    "        model = ResNet50(weights='imagenet', pooling=pooltype, include_top=False)       \n",
    "    if ('InceptionV3' is model_name):\n",
    "        model = InceptionV3(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('Xception' is model_name):\n",
    "        model = Xception(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('InceptionResNetV2' is model_name):\n",
    "        model = InceptionResNetV2(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('NASNetMobile' is model_name):\n",
    "        model = NASNetMobile(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('NASNetLarge' is model_name):\n",
    "        model = NASNetLarge(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('DenseNet121' is model_name):\n",
    "        model = DenseNet121(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('DenseNet169' is model_name):\n",
    "        model = DenseNet169(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "    if ('DenseNet201' is model_name):\n",
    "        model = DenseNet201(weights='imagenet', pooling=pooltype, include_top=False)\n",
    "\n",
    "    if (model_name in ['InceptionV3','Xception','InceptionResNetV2']):\n",
    "        targetSize = 299  \n",
    "    elif(model_name in ['NASNetLarge']):\n",
    "        targetSize = 331\n",
    "    else:    \n",
    "        targetSize = 224   \n",
    "\n",
    "    return model, targetSize\n",
    "\n",
    "# # NEW - TEST\n",
    "# def normalize(image_array):\n",
    "#     imagenet_means = np.array([0.485, 0.456, 0.406])\n",
    "    \n",
    "#     pixels = np.asarray(image_array).astype('float32')\n",
    "#     pixels /= 255.0\n",
    "#     pixels -= imagenet_means\n",
    "    \n",
    "#     return pixels\n",
    "\n",
    "# Load the image and predict features with chosen model\n",
    "def extractDeepFeatures(file_id, model, model_name, target_size, log=False):   \n",
    "    x = load_img(file_id,target_size)\n",
    "    x = image.img_to_array(x)\n",
    "#     x = normalize(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    if (model_name is 'VGG16'):\n",
    "        x = ppi_vgg16(x)        \n",
    "    if (model_name is 'VGG19'):\n",
    "        x = ppi_vgg19(x)\n",
    "    if (model_name is 'ResNet50'):\n",
    "        x = ppi_resnet50(x)\n",
    "    if (model_name is 'MobileNet'):\n",
    "        x = ppi_mobilenet(x)\n",
    "    if (model_name is 'Xception'):\n",
    "        x = ppi_xception(x) \n",
    "    if (model_name is 'InceptionV3'):\n",
    "        x = ppi_inception_v3(x)\n",
    "    if (model_name is 'InceptionResNetV2'):\n",
    "        x = ppi_inceptionresnet_v2(x)\n",
    "    if (model_name is 'NASNetMobile'):\n",
    "        x = ppi_nasnet(x)\n",
    "    if (model_name is 'NASNetLarge'):\n",
    "        x = ppi_nasnet(x)\n",
    "    if (model_name is 'DenseNet121'):\n",
    "        x = ppi_densenet(x)\n",
    "    if (model_name is 'DenseNet169'):\n",
    "        x = ppi_densenet(x)\n",
    "    if (model_name is 'DenseNet201'):\n",
    "        x = ppi_densenet(x)\n",
    "\n",
    "    timeStart = current_milli_time()\n",
    "    features = model.predict(x)\n",
    "    process_time = current_milli_time() - timeStart    \n",
    "    if(log): print('Load process: Complete', process_time)\n",
    "\n",
    "    return features, process_time, x\n",
    "\n",
    "# Load each sample image\n",
    "def load_img(filepath, target_size): \n",
    "    if (filepath.endswith('jpg') or filepath.endswith('JPG') ):\n",
    "        img = cv2.imread((filepath))\n",
    "    elif (filepath.endswith('txt')):\n",
    "        img = np.loadtxt(filepath).copy()\n",
    "    elif (filepath.endswith('npy')):\n",
    "        img = np.load(filepath).copy()\n",
    "    elif (filepath.endswith('png') or filepath.endswith('PNG')):\n",
    "        img = cv2.imread(filepath)\n",
    "    img = cv2.resize(img, (target_size, target_size), interpolation = cv2.INTER_NEAREST)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 - Extract deep features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_extractor(model_name, classes_list, output_fmt=['npy'], srcPath='./data', outPath='./out'):\n",
    "    \n",
    "    print(model_name)\n",
    "    cont = 0\n",
    "    csv_aux = 0\n",
    "\n",
    "    model, targetSize = build_model(model_name)\n",
    "    data = []\n",
    "    process_time_list= []\n",
    "\n",
    "    # Start chronometer\n",
    "    timeStart = current_milli_time()\n",
    "\n",
    "    # Range: classes\n",
    "    for fileIdx in classes_list:\n",
    "        folderPath = srcPath + os.sep + str(fileIdx)\n",
    "        print(folderPath)\n",
    "\n",
    "        # For each sample\n",
    "        for subdir, dirs, files in os.walk(folderPath):\n",
    "            for idx,name in enumerate(files):\n",
    "                filePath = subdir + os.sep + name\n",
    "                cont=cont+1\n",
    "                if ImagesID:\n",
    "                    id_img = name.split('_')[0]\n",
    "\n",
    "                clear_output()             \n",
    "                print(str(cont) + ' | ' + filePath + ' | ' + model_name) \n",
    "\n",
    "                features, process_time, processed_img = extractDeepFeatures(filePath, model,model_name, targetSize)\n",
    "                process_time_list.append(process_time)\n",
    "                   \n",
    "                # CSV file has a advantage: it can save the name, or ID, of image (string type)\n",
    "                if ('csv' in output_fmt):\n",
    "                    if csv_aux == 0:\n",
    "                        features_df = np.append(features, fileIdx)\n",
    "                        features_df = np.expand_dims(features_df, axis=0)\n",
    "                        data_df = pd.DataFrame(data=features_df)\n",
    "                        image_name = pd.DataFrame(data=np.array([name]))\n",
    "                        data_df_csv = pd.concat([image_name, data_df], axis=1)\n",
    "                        csv_aux+=1\n",
    "                    else:\n",
    "                        features_df2 = np.append(features, fileIdx)\n",
    "                        features_df2 = np.expand_dims(features_df2, axis=0)\n",
    "                        data_df = pd.DataFrame(data=features_df2)\n",
    "                        image_name = pd.DataFrame(data=np.array([name]))\n",
    "                        data_df = pd.concat([image_name, data_df], axis=1)  \n",
    "                        data_df_csv = pd.concat([data_df_csv, data_df], axis=0)\n",
    "                        \n",
    "                # The first column of each line is the image id, the last column is the class of image\n",
    "                features = features.reshape(-1)\n",
    "                features = np.hstack((features, fileIdx))\n",
    "                if ImagesID:\n",
    "                    features = np.hstack((int(id_img), features))\n",
    "                data.append(features)\n",
    "\n",
    "    # Compute extraction time      \n",
    "    process_time = current_milli_time()-timeStart    \n",
    "\n",
    "    # Saves a txt file w/ deep features\n",
    "    if('txt' in output_fmt):\n",
    "        np.savetxt(outPath+os.sep+model_name+'.txt', data, fmt=\"%.8f\")\n",
    "\n",
    "    # Saves a npy file w/ deep features\n",
    "    if('npy' in output_fmt):\n",
    "        np.save(outPath+os.sep+model_name+'.npy', data) \n",
    "    \n",
    "    # Saves a csv file w/ deep features and w/ images identifiers\n",
    "    if('csv' in output_fmt):\n",
    "        data_df_csv.to_csv(outPath+os.sep+model_name+'.csv')\n",
    "\n",
    "    # Save process time into a file\n",
    "    np.savetxt(outPath+os.sep+model_name+'_time.txt', process_time_list, fmt=\"%d\")\n",
    "\n",
    "    return process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 - Extraction general**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6817 | ./dataset/DDSM_ROI/mass&B2&B5_augmentation/5/Mass-Training_P_00568_LEFT_CC_1._rot67.jpg | VGG16\n",
      "Load process: Complete 756013\n",
      "\n",
      "\n",
      "Done\n",
      "CPU times: user 11min 23s, sys: 2min 48s, total: 14min 12s\n",
      "Wall time: 12min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Creates a folder if don't exist\n",
    "outPathF = './results/features'\n",
    "outPathC = './results/classification'\n",
    "os.makedirs('./results/features')\n",
    "os.makedirs('./results/classification/metrics')\n",
    "os.makedirs('./results/classification/plot')\n",
    "os.makedirs('./results/classification/rounds')\n",
    "\n",
    "## For each deep model    \n",
    "for model_name in model_name_list:\n",
    "    process_time = deep_extractor(model_name, classes, output_fmt, srcPath, outPathF)\n",
    "    print('Load process: Complete', process_time)\n",
    "\n",
    "print('\\n\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 - Initialize classifiers and parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "clf_list = np.array([MultinomialNB(),        \n",
    "                     MLPClassifier(max_iter=1000, solver='adam', learning_rate_init=5e-04),      \n",
    "                     KNeighborsClassifier(),   \n",
    "                     RandomForestClassifier(class_weight=class_weight),\n",
    "                     svm.SVC(kernel='linear', class_weight=class_weight, probability=True, max_iter=3000, tol=1e-3),\n",
    "                     svm.SVC(kernel='poly', class_weight=class_weight, probability=True, max_iter=3000, tol=1e-3),\n",
    "                     svm.SVC(kernel='rbf', class_weight=class_weight, probability=True, max_iter=3000, tol=1e-3)\n",
    "                     ])\n",
    "\n",
    "# Specify parameters and distributions to classifiers\n",
    "param_dist_list = np.array([# Bayes\n",
    "                                None, \n",
    "                            # MLP\n",
    "                                {\"hidden_layer_sizes\": list(np.arange(2,1001))},\n",
    "                            # KNN\n",
    "                                {\"n_neighbors\": [1,3,5,7,9,11]}, \n",
    "                            # Random Forest                   \n",
    "                                {\"n_estimators\": [3000],\n",
    "                                 \"max_depth\": [6, None],\n",
    "                                 \"max_features\": sp_randint(1, 11),\n",
    "                                 \"min_samples_split\": sp_randint(2, 11),\n",
    "                                 \"min_samples_leaf\": sp_randint(1, 11),\n",
    "                                 \"bootstrap\": [True, False],\n",
    "                                 \"criterion\": [\"gini\", \"entropy\"]},\n",
    "                            # SVM Linear\n",
    "                                {'kernel': ['linear'], 'C': [2**i for i in range(-5,15)]},\n",
    "                            # SVM Polynomial    \n",
    "                                {'kernel': ['poly'], 'degree': [3, 5, 7 ,9], 'C': [2**i for i in range(-5,15)]},                    \n",
    "                            # SVM RBF    \n",
    "                                {'kernel': ['rbf'], 'gamma': [2**i for i in range(-15,3)], \\\n",
    "                                 'C': [2**i for i in range(-5,15)]}\n",
    "                            ])\n",
    "\n",
    "# Numbers idx of each classifier\n",
    "clf_numbers = {'Bayes':0, 'MLP':1, 'Nearest_Neighbors':2, 'Random_Forest':3, 'SVM_Linear':4, \\\n",
    "               'SVM_Polynomial':5, 'SVM_RBF':6}\n",
    "\n",
    "# Configure list and parameters of classifiers chosen\n",
    "idx_clf = np.array([clf_numbers[i] for i in classifiers_name_list])\n",
    "clf_list = clf_list[idx_clf]\n",
    "param_dist_list = param_dist_list[idx_clf]\n",
    "\n",
    "# Configure interations of 'Random Search'\n",
    "n_iter_search_list = np.array([0,20,5,15,20,20,20])\n",
    "n_iter_search_list = n_iter_search_list[idx_clf]\n",
    "cv = 3 # Random Search K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 - Process data functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data per class\n",
    "def split(ds, classes):\n",
    "    dsx = []\n",
    "    for class_id in classes:\n",
    "        dsi = ds[ds[:, -1] == class_id]\n",
    "        dsx.append(dsi)\n",
    "    return dsx\n",
    "\n",
    "# Load data for train-test from dataset INBreast\n",
    "def load_train_test(ds, ntr, nte, classes):  \n",
    "    list_ds = split(ds, classes)\n",
    "    \n",
    "    for i, dsi in enumerate(list_ds):\n",
    "        if i == 0:\n",
    "            train = dsi[:ntr[i],:]\n",
    "            test = dsi[ntr[i]:ntr[i]+nte[i],:]\n",
    "        else:\n",
    "            train = np.concatenate((train, dsi[:ntr[i],:]), axis=0)\n",
    "            test = np.concatenate((test, dsi[ntr[i]:ntr[i]+nte[i],:]), axis = 0)\n",
    "            \n",
    "    X_train = np.array(train[:, :-1])\n",
    "    y_train = np.array(list(map(int, train[:,-1])))\n",
    "    X_test = np.array(test[:, :-1])\n",
    "    y_test = np.array(list(map(int, test[:, -1])))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Normalize data\n",
    "def normalizeX(X):        \n",
    "    X = np.array(X - np.mean(X))\n",
    "    X = np.array(((X - np.min(X)) / (np.max(X) - np.min(X))))     \n",
    "    return X\n",
    "\n",
    "# Shuffle data\n",
    "def shuffle(x_data, y_data):\n",
    "    c = list(zip(x_data, y_data))\n",
    "    random.shuffle(c)\n",
    "    X, y = zip(*c)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9 - Classification general - NEED TO EDIT METRICS CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: VGG16 deep model\n",
      "\tprocessing: round 1\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tprocessing: round 2\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tprocessing: round 3\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n",
      "\tprocessing: round 4\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tprocessing: round 5\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tprocessing: round 6\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n",
      "\tprocessing: round 7\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n",
      "\tprocessing: round 8\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tprocessing: SVM_RBF classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus-a-s/.MyEnv/lib/python3.5/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tprocessing: round 9\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n",
      "\tprocessing: round 10\n",
      "\t\tprocessing: Nearest_Neighbors classifier\n",
      "\t\tprocessing: SVM_Linear classifier\n",
      "\t\tprocessing: SVM_RBF classifier\n",
      "Load process: Complete 48507\n",
      "\n",
      ".......Loading: New Model.......\n",
      "\n",
      "\n",
      "\n",
      "Done\n",
      "CPU times: user 1min 13s, sys: 2.43 s, total: 1min 16s\n",
      "Wall time: 14min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model_name in model_name_list: # Iterate CNN models\n",
    "    print('processing: '+model_name+' deep model')\n",
    "    \n",
    "    header_results = ['round', 'classifier', 'process_time']\n",
    "    header_results[2:2] = metrics_name_list\n",
    "    \n",
    "    with open(outPathC+os.sep+'metrics'+os.sep+model_name+'_results.csv', mode='a') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow(header_results)\n",
    "\n",
    "    data = np.load(outPathF+os.sep+model_name+\".npy\")\n",
    "    list_df = []\n",
    "\n",
    "    for rnd in range(classification_rounds): # Iterations of rounds\n",
    "\n",
    "        print('\\tprocessing: round '+str(rnd+1))\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "        X_train, y_train, X_test, y_test = load_train_test(data, set_split['train_set'], set_split['test_set'], classes)      \n",
    "        X_train, y_train  = shuffle(X_train, y_train)\n",
    "        X_test, y_test  = shuffle(X_test, y_test)\n",
    "        \n",
    "        if ImagesID:\n",
    "            id_test = X_test[:,0].reshape(-1)\n",
    "            X_test = X_test[:,1:]\n",
    "            X_train = X_train[:,1:]\n",
    "\n",
    "        X_train = normalizeX(X_train)        \n",
    "        X_test = normalizeX(X_test) \n",
    "\n",
    "        with open(outPathC+os.sep+'rounds'+os.sep+model_name+'_round'+str(rnd+1)+'.csv','a') as csv_predict:\n",
    "            csv_writer = csv.writer(csv_predict, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerow(['test_img'+str(i) for i in range(sum(set_split['test_set']))])\n",
    "            if ImagesID:\n",
    "                csv_writer.writerow(['test_id', np.asarray(id_test)])\n",
    "            csv_writer.writerow(['test_class', np.asarray(y_test)])\n",
    "\n",
    "        for idx, (clf,clf_name,param_dist,n_iter_search) in enumerate(zip(clf_list, classifiers_name_list, \\\n",
    "                                                                          param_dist_list, n_iter_search_list)): \n",
    "            # Start chronometer\n",
    "            timeStart = current_milli_time()\n",
    "\n",
    "            print('\\t\\tprocessing: '+clf_name+' classifier')\n",
    "\n",
    "            if(param_dist != None): \n",
    "                random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, \\\n",
    "                                                   n_jobs=-1, cv=cv)\n",
    "                random_search.fit(X_train, y_train)\n",
    "                clf = random_search.best_estimator_                    \n",
    "            else:\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "            predict = clf.predict(X_test)\n",
    "\n",
    "            with open(outPathC+os.sep+'rounds'+os.sep+model_name+'_round'+str(rnd+1)+'.csv','a') as csv_predict:\n",
    "                csv_writer = csv.writer(csv_predict, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(['predict_'+clf_name, np.asarray(predict)])\n",
    "                \n",
    "            # Compute metrics\n",
    "            accuracy = accuracy_score(y_test, predict)\n",
    "            bal_accuracy = balanced_accuracy_score(y_test, predict)\n",
    "            precision = precision_score(y_test, predict, average='macro')\n",
    "            sensitivity = sensitivity_score(y_test, predict, average='macro')\n",
    "            specificity = specificity_score(y_test, predict, average='macro')\n",
    "            f1 = f1_score(y_test, predict, average='macro')\n",
    "            \n",
    "            # End chronometer\n",
    "            process_time = current_milli_time() - timeStart\n",
    "\n",
    "            # Generate confusion matrix and save per round\n",
    "            cmx = confusion_matrix(y_test, predict)\n",
    "            cmx_cols = classes\n",
    "            cmx_rows = classes\n",
    "            csv_matrix = pd.DataFrame(cmx, index=cmx_rows, columns=cmx_cols)\n",
    "            csv_matrix.to_csv(outPathC+'/rounds/'+model_name+'_CMX'+str(rnd+1)+'_'+clf_name+'.csv', sep=',')\n",
    "\n",
    "            row = [rnd+1, clf_name, accuracy*100, bal_accuracy*100, precision*100, sensitivity*100, \\\n",
    "                   specificity*100, f1*100, process_time]\n",
    "\n",
    "            with open(outPathC+os.sep+'metrics'+os.sep+model_name+'_results.csv', mode='a') as csv_file:\n",
    "                csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "    print('Load process: Complete', process_time)\n",
    "    print(\"\\n.......Loading: New Model.......\\n\")\n",
    "\n",
    "print('\\n\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 - Read CSV of Results and Compute Means and STD's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_name_list:\n",
    "    metrics_csv = pd.read_csv(outPathC+os.sep+'metrics'+os.sep+model_name+'_results.csv', sep=\",\")\n",
    "\n",
    "    means = metrics_csv.groupby(['classifier']).mean()\n",
    "    stds = metrics_csv.groupby(['classifier']).std()\n",
    "\n",
    "    classifiers = means.index.get_level_values(0)\n",
    "    classifiers = np.resize(np.asarray(classifiers),(1,len(classifiers_name_list))) # 1 and classifiers\n",
    "    classifiers = classifiers.T\n",
    "\n",
    "    means = np.asarray(means)\n",
    "    means = means[:,1:]\n",
    "    stds = np.asarray(stds)\n",
    "    stds = stds[:,1:]\n",
    "\n",
    "    new_csv = np.concatenate((classifiers, means), axis=1)\n",
    "    new_csv = np.concatenate((new_csv, stds), axis=1)\n",
    "\n",
    "    cab = ['classifier', 'accuracy_mean', 'balanced_accuracy_mean', 'precision_mean', 'sensitivity_mean', \\\n",
    "           'specificity_mean', 'f1_score_mean', 'process_time_mean', 'accuracy_std', 'balanced_accuracy_std', \\\n",
    "           'precision_std', 'sensitivity_std', 'specificity_std', 'f1_score_std', 'process_time_std']\n",
    "\n",
    "    new_csv = pd.DataFrame(new_csv, columns=cab).to_csv(outPathC+os.sep+'metrics'+os.sep+model_name+'_mean_std.csv', \\\n",
    "                                                        sep=',', index=False)\n",
    "\n",
    "    pd.read_csv(outPathC+os.sep+'metrics'+os.sep+model_name+'_mean_std.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11 - Generate Graphics of Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{0:.2f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "for model_name in model_name_list:\n",
    "    plot_csv = pd.read_csv(outPathC+os.sep+'metrics'+os.sep+model_name+'_mean_std.csv', sep=',')\n",
    "\n",
    "    x_pos = np.arange(len(plot_csv))\n",
    "    x_pos = np.arange(1, 2*x_pos.shape[0]+1, 2)\n",
    "    materials = classifiers_name_list\n",
    "    n_classifiers = len(classifiers_name_list)\n",
    "\n",
    "    width = 0.15\n",
    "\n",
    "    lines = list(range(len(plot_csv)))\n",
    "\n",
    "    means = [1,2,3,4,5,6]\n",
    "    stds = [8,9,10,11,12,13]\n",
    "\n",
    "    CTEs = []\n",
    "    errors = []\n",
    "\n",
    "    # Metrics\n",
    "    for i in range(len(metrics_name_list)):\n",
    "        CTE = [plot_csv.iloc[lines[j],means[i]] for j in range(n_classifiers)]\n",
    "        error = [plot_csv.iloc[lines[j],stds[i]] for j in range(n_classifiers)]\n",
    "\n",
    "        CTEs.append(CTE)\n",
    "        errors.append(error)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    r1 = ax.bar(x_pos - width - 0.3, CTEs[0], yerr=errors[0], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='Acurácia')\n",
    "    r2 = ax.bar(x_pos - 0.3 , CTEs[1], yerr=errors[1], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='Acurácia Balanceada')\n",
    "    r3 = ax.bar(x_pos + width - 0.3, CTEs[2], yerr=errors[2], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='Precisão')\n",
    "    r4 = ax.bar(x_pos + 2*width - 0.3, CTEs[3], yerr=errors[3], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='Sensitividade')\n",
    "    r5 = ax.bar(x_pos + 3*width - 0.3, CTEs[4], yerr=errors[4], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='Especificidade')\n",
    "    r6 = ax.bar(x_pos + 4*width - 0.3, CTEs[5], yerr=errors[5], width=width,align='center', alpha=0.5, ecolor='black', \\\n",
    "                capsize=3, label='F1-Score')\n",
    "\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_yticks(np.arange(0,110,10))\n",
    "    ax.set_xticks(x_pos)\n",
    "#     ax.set_xticklabels(materials)\n",
    "    ax.set_xticklabels(['KNN', 'SVM Linear', 'SVM RBF'])\n",
    "    ax.set_title('Resultados da Classificação: '+model_name)\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    autolabel(r1)\n",
    "    autolabel(r2)\n",
    "    autolabel(r3)\n",
    "    autolabel(r4)\n",
    "    autolabel(r5)\n",
    "    autolabel(r6)\n",
    "\n",
    "    # Save the figure and show\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outPathC+os.sep+'plot'+os.sep+model_name+'_plot.png')\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MyEnv",
   "language": "python",
   "name": ".myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
